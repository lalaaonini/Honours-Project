\chapter{Feature Exploration and Extraction}
\label{ch:Features}

\section{Types of Features}
When data mining in time series,  consider each point in time sequentially is not usually sufficient. Asside from ignoring the high dimensionality of the data,  it does not account for the correlation between consecutive values \cite{Moerchen}. It is therefore beneficial to transform and aggregate the data in such a way as to reduce the dimensionality as well as capture differences in consumption patterns between classes. 

According to Beckel et al. \cite{Beckel_2}, features of interest when classifying households based on energy consumption would include: consumption figures, ratios, temporal properties, and statistical properties. Consumption figures represent the average, maximum and minimum amount of energy used over a defined period of time. Ratios are features that calculate the fraction between the consumption figures; they capture relevant patterns that occur across points in time. Temporal features express the first or last occurrence of some event, the time at which the daily maximum or minimum energy demand takes place, or any periodicity within the household's electricity consumption. Finally, statistical properties, such as variances or correlations, give insight into the consumption curve and demonstrate how a household's activity may fluctuate throughout a predetermined time period.

\section{Non-Linear Transformation}
Numerous statistical methods presume that input data follows a normal distribution. With this in mind, the HES data was visualized and compared against a normal quantile plot to find suitable non-linear transformations \cite{Osborne} \cite{Wang}. Figure \ref{fig:qqPlot} shows the normal quantile plot of the average standard deviation of a household on Mondays (left) and the logarithm of this feature (right). The linearity of the sample quantiles of the features (x-axis) versus the theoretical quantiles of a normal distribution (y-axis) implies that the transformed features are (roughly) normally distributed. These transformations are important for classifiers, such as k-nearest neighbour, which rely on the distance between samples based on their features.

%\qqPlot


\section{Creating Features}
\label{sec:creatingFeatures}
One method of extracting features is to compute as many different types as possible, compare them all and then choose those that best discriminate between classes. Applying this to households, in addition to considering the energy used over a month, data could be further split into weeks, days and even hours. Consumption figures and statistical properties would then be calculable for each of these intervals. While this method does provide more coverage, and thus a greater probability of finding the optimal features to discriminate between classes, it ignores any domain knowledge that we might already have. As such, it is potentially wasteful of the limited resources available for the project at hand. 

Focusing on efficiency, instead of computing features in an ad hoc manner, they were created as follows: 1) Assumptions were made regarding the distinction between classes (e.g., households with children use more energy overall). 2) Features were created to capture this distinction (e.g., the average energy over a 4-week period). 3) Tests were performed to evaluate the validity of the assumption (e.g., visualising the feature). The tests varied in thoroughness as it was sometimes obvious from visualisation alone that they discriminated between classes. At other times, more sophisticated methods were needed, as described in Section \ref{sec:dimensionalityReduction}. %\featureSelectionSection.

The remainder of this chapter describes features that were created from the energy reading data and justifies why it was assumed that they would discriminate between classes. The results of computing these features are then evaluated. Both classification problems (socio-economic classification and child classification) were considered when choosing features to evaluate.

\subsection*{Total Electricity}
In visualising the HES data, it became evident that there were large disparities in energy usage among households. While some had an average smart meter reading of 109Wh, others averaged as little as 40Wh\footnote{The smart meters actually measured in units of 0.1Watt hours}. One household was recorded to have consumed as much as 3.25kWh in 10 minutes (3,250Wh); another never used more than 198Wh over the same time interval. To determine whether these discrepancies could be attributed to different classes, the first feature that was explored was the total energy consumed within a given period of time. This was chosen to be  28 days (for reasons outlined in Section \ref{sec:preprocessing}). 

Building a classifier using the total electricity as input assumes that some classes use more energy than others. This can be justified as correlations between a household's disposable income and the amount of energy it uses have been observed in previous studies \cite{Gomez}.

\monthSum %good

Looking at Figure \ref{fig:monthSum}, there appears to be a distinction between classes in total electricity consumption. The left hand plot, which compares households with children against those without, indicates that those with tend to use more energy. The right-hand plot, which compares total electricity according to social grade, shows that the highest socio-economic households use more energy than those of the lowest social grade. It does not, however, discriminate well between intermediate classes.

\subsection*{Average Daily Usage}
\hl{Having established that some classes of households do indeed use more energy than others, the average energy used by each household on each day of the week can be computed to determine the underlying cause}. This sort of feature explores not just whether some classes use more electricity than others, but if the electricity consumed is dependent on the day of the week. This was computed by taking the total energy used on each day, grouping similar days together (based on which day of the week they are), and then taking the average of each group.

\averageDayChild %good
\averageDaySocio %good

Figure \ref{fig:averageDayChild} further illustrates that households with children use more power than those without. It does not, however, provide any additional insight as to when, how or why this is the case. Households with children tend to use more electricity per day, irrespective of what day of the week it is.

Similarly, Figure \ref{fig:averageDaySocio}, which compares the average daily usage of among socio-economic groups, does not offer any new insights into the differences between classes. There is no particular day where differences in electricity consumption between classes are more visible.

It should be noted, that due to the way the data was pre-processed in Section \ref{sec:preprocessing}, there is an inherent bias in features that rely on average consumption over the four-week period. If a day had to be reused in order to ensure 28 days worth of data, then it is counted twice when taking the average. 

\subsection*{Average Part-Of-Day (APOD)}
\hl{Going further, different classes potentially use more or less energy at different times of the day}. For example, lower socio-economic households might use more energy during the day than those of medium or high socio-economic status. This is because the conditional probability of unemployment given lower socio-economic status is higher than that of unemployment given higher socio-economic status \cite{Bartley}. Similarly, it would not be unreasonable to assume that the consumption gap between households with and without children might shrink when the children are at school and widen when they are at home. APOD (Average Part-Of-Day) features are computed by grouping the smart meter readings according day-of-week (as in the previously computed feature), as well as by part-of-day. The resulting averages of each group is then taken to be a feature. As detailed below, it makes sense to look at days as the sum of four parts, resulting in 28 new features.

Most schools days in England begin at 9:00 and finish between 15:00 and 16:00 \cite{school_times}. Combining this information with  an assumption that as children go to bed, the activity of the other members of a household will decrease and therefore electricity consumption will drop, it is worthwhile to split each day into the following groups.
\begin{enumerate}
\item Morning (6:00-9:00): The time when members of the household wake up and prepare themselves for work, school etc.
\item Daytime (9:00-15:00): The time children are at school.
\item Evening (15:00-22:00):  When a household can be presumed to be most active
\item Night (22:00-6:00): Depending on the type of household, the time when people might be more or less active. For example, couples without children might stay up later.
\end{enumerate}
\APODChild %good


The data portrayed in Figure \ref{fig:APODChild} indicates that energy use patterns are indeed different for households with versus without children. Much of the difference can be attributed to household activity in the Evenings (15:00-22:00). It can also be seen that on weekday Daytime (9:00-15:00, Monday-Friday), the two classes use similar amounts of electricity. However on Saturdays and Sundays, the gap widens and those with children tend to use more than those without. 

\APODSocio %good

Figure \ref{fig:APODSocio}, which compares socio-economic classes, shows again the same distribution as the previously computed features. Households of social grade E appear to use relatively little energy at night compared to households of other socio-economic groups, yet they seem to make up for it in the morning period, when their consumption is more akin to the other groups. Households of class A show the opposite pattern, using more energy than the others in the evenings, but normal amounts (compared to other classes) in the mornings.

Based on the observations thus far, it is likely that a classifier would have difficulty differentiating between socio-economic classes, particularly when discriminating between households of classes B or D and the rest. They have they have lower prior probabilities than C1 and C2, and do not look to have the extremes in energy usage that classes A and (particularly) E do. 


\subsection*{Mean Weekday vs. Saturday and Sunday}
In addition to looking at consumption features, ratios can also give insight into when a household is using its energy. Taking the ratio of the energy consumed on a weekend day to weekdays, one can determine if a household is using proportionately more of its energy during the week or at the weekends. Households of social grades E, D and C2, whose chief income earners, when employed, are often either unskilled or manual labourers, or work in lower level positions within such industries as retail, hospitality and food services, are more likely to be required to work at the weekends than households of classes C1, B or A who, given their supervisory or managerial roles, are less likely to work at the weekends \cite{careers}. The feature is computed by taking the average energy consumed by a household on a Saturday and a Sunday, and dividing that by the average energy used during the week\footnote{Simply taking the total energy used on all Saturdays and Sundays and dividing it by the total used on weekdays would give the same result. However, the decision was taken to use the average because these values were conveniently available from previously computed features.}. The features can be expressed as:

\begin{align*}
\text{Sat Ratio}&=\frac{\text{mean total energy used on Sat}}{\text{mean total energy used from Mon to Fri}}\equiv\frac{\sum_{i=1}^4Sat_i}{\sum_{i=1}^4{Mon_i + Tue_i+\cdots+ Fri_i}} \\
\text{Sun Ratio}&=\frac{\text{mean total energy used on Sun}}{\text{mean total energy used from Mon to Fri}}\equiv\frac{\sum_{i=1}^4Sat_i}{\sum_{i=1}^4{Mon_i + Tue_i+\cdots+ Fri_i}} 
\end{align*}
where $Day_i$ is the total energy used on the $i^{th}$ occurence of that day over the four-week period
\POWrat

After computing the ratio between weekend and weekday electricity consumption, classes seem to use similar proportions of their energy. And while Figure \ref{fig:POWrat} suggests that households generally use more of their energy on Sundays than on Saturdays, this is independent of whether or not the household has children. Households of socio-economic class A are found to use a lower proportion of their total energy on Saturdays than other classes, but might use more on Sundays, whereas the opposite is true for households of class E. Again, there is the issue that households of classes D,C1,C2 and B are indistinguishable from one another based on the plot.


\subsection*{Variance on Weekdays}
Thus far, the features that have been computed have been dependent on \textit{how much} energy has been consumed. It is also worth considering what can be inferred from the volatility of a household's energy consumption over the course of a day. The assumption behind such a feature is that certain types of households will have more `spikes' in their daily consumption. For example, households with greater disposable income (which is correlated with their socio-economic status) are more likely to own appliances that have high energy demands such as tubule driers and dishwashers.

\logADVChild %good


Although the average daily variance of households is volatile in and of itself, the results shown in Figure \ref{fig:logADVChild} indicate that the electricity use of households with children does tend to fluctuate more than those without children and therefore could be used to discriminate between households with and without children. 

\logADVSocio %good

According to Figure \ref{fig:logADVSocio}, it is conceivable that the variance of a household's electricity consumption could be used to determine the socio-economic class of the household. For example, households of class E can be distinguished from the remaining classes based on the variance of their consumption on Thursdays and Fridays. On Tuesdays, households of class B stand apart from A, C2, C1 and D. %However, the range of of the features is itself relatively large and there are numerous outliers (represented by the red dots) which will will affect the classifiers in a negative way. 


\subsection*{Correlation Between Weekdays}

The final feature calculated from the consumption figures is the average correlation coefficient between one weekday and every other weekday.  Each weekday was taken turn, and the correlation between it and each subsequent weekday was calculated. This is done for each week separately and the averaged correlation of each pair of days is considered as one feature. Rather than using the 10-minute intervals, which appeared to be too granular to capture any covariance between days, electricity readings were summed into one-hour intervals. This is done because, while people might have certain tasks that they repeat daily (such as having a cup of tea), it is unreasonable to assume that it is always done during the same 10 minute period each day, but likely to be within the same hour.  



\corrChild %good

Looking at Figure \ref{fig:corrChild}, it appears that although the correlation coefficients are generally close to 0 (meaning there is no correlation), there are nevertheless differences between the two classes. Depending on which two days are being considered, the correlations of one class tend to be greater or smaller than that those of the others. For example, it would appear that households with children demonstrate a slightly higher correlation between their Monday and Tuesday electricity consumption patterns than those without. Whereas for socio-economic classification, as depicted in Figure \ref{fig:corrSocio}, the correlation between days does not result in features that separate classes.

\corrSocio %good


\section{Periodicity}

Another approach used for feature extraction is to exploit the periodic consumption patterns exhibited by many households in order to search for temporal structures present in some classes but not in others. This method of feature extraction has been used successfully in previous studies involving forecasting and clustering. Methods outlined by Fabian Moerchen \cite{Moerchen} for time series feature extraction are used to project each household's consumption into the frequency domain from which the most important frequencies are found. McLoughlin et al. \cite{McLoughlin} showed in their research that temporal structure is present in household electricity consumption data and can be used to characterise domestic energy demand.


\subsection*{Signal Smoothing}

Before projecting the electricity consumption into frequency space, the Gaussian averaging operator was applied to each set of readings to filter noise whilst retaining the temporal structure of the data. Gaussian filtering (or Gaussian smoothing) is accomplished by convolving a time series with the Gaussian function. It can improve performance compared with direct averaging, as more structure is retained whilst noise is removed \cite{Nixon}. This is done because the time-frequency transformation used (the discrete Fourier transform method) has difficulty characterising small intervals of large electricity demand \cite{Graps}.



\householdConv %good



\subsection*{Fourier Transform}

For uniform samples $[f(1)...,f(n)]$ of a real signal $f(x)$, the \textit{Discrete Fourier Transform} (DFT) is the projection of a signal from the time domain into the frequency domain by

\[c_f=\frac{1}{\sqrt{n}}\sum_{t=1}^nf(t)\exp{\frac{-2\pi ift}{n}}\]
where $f=1,...n$ and $i=\sqrt{-1}$. The $c_f$ are complex numbers and represent
the amplitudes and shifts of a decomposition of the signal into sinusoid functions \cite{Moerchen}.


Issues do present themselves when using this method. As already mentioned, the Fourier transform measures global frequencies and the signal is assumed to be periodic. This assumption can cause poor approximations at the borders of the time series \cite{Moerchen}.

\subsection*{Energy Preservation}

For $l$ time series of length $m$, the DFT produces an $l \times m$ matrix $C $ of coefficients, such that element $c_{i,j}$ is the $j^{th}$ coefficient of time series $i$. In our case, since the number of households, $l=519$, is small compared to the length of each time series, $m=4032$, the number of coefficients must be reduced in order to minimise redundancy, noise and computational time. According to Moerchen \cite{Moerchen}, the best subset of $k$ columns is found by selecting those that optimize energy preservation $E$, defined as

\[E(f(t))=\sum_{j=1}^ma_jc^2_j\] 
where $c_j$ is the $j^{th}$ column and $a_j$ is an appropriate scaling coefficient corresponding to signal $f(t)$. 

Let $I$ be a function measuring the importance of coefficient $j$ on all values of $l$, and let $J_k(I,C)$ be a function that chooses a subset of $M = {1, ..., m}$ of the $k$ largest values of $I$. Moerchen \cite{Moerchen} proves that $J_k(mean(c_j^2),C)$ is optimal in energy preservation.

The MATLAB fast Fourier transform function (fft) was used to find the discrete Fourier transform; the five best features were chosen based on the energy preservation method.


		
\section{Dimensionality Reduction}
\label{sec:dimensionalityReduction}

Even though the success of a classifier is dependent on several variables, which may differ from one classifier to another, all classifiers are dependent on the quality of their input data. To achieve accurate results with the least amount of computational time, it is necessary to ensure that the minimal amounts of noise and redundancy is present in the input. This may involve dimensionality reduction, the process of identifying and filtering out as much irrelevant and redundant information as possible \cite{Hall}. 

Different classification algorithms will be affected by overparameterisation in different ways. In the K-nearest neighbour classifier, additional features can largely affect the distance between two points. While redundant features (i.e., those that don't change the distance between points) would only influence computational cost, added noise to the system can impact the distance between points, likely in a negative way. 

Like K-nearest neighbour, the need for feature reduction in logistic regression has less to do with removing redundancy than with reducing noise and computational cost. Logistic regression accounts for highly correlated features by lowering their weights. Uninformative features, however, would cause weights to be learned that do not improve the performance of the classifier. 

Random forests are not as susceptible to the problem of overparameterisation as other methods. When training each tree, since the `best' features will be branched on towards the top of the tree, pruning could be used to limit the size of each tree (thus avoiding overfitting). An issue would only start to arise when the number of redundant or noisy features become much larger than the number of good features. This is because, when training a tree, a random subset of features is selected as each branch is created. If the number of `bad' features is much larger than the number of `good' ones, then the probability of choosing a subset where no good features are present becomes significant.

Dimensionality reduction can usually be characterised as one of two tasks: \textit{feature selection} and \textit{feature transformation}. Feature transformation methods involve performing a transformation of the data (such as a rotation or projection) to create a new set of features (of smaller size) that has more descriptive power than the original set. A commonly used example of this is \textit{principal component analysis} (PCA) which finds a set of orthogonal unit vectors that point in the directions of greatest variance of the data. The features are given by projecting the data onto this basis. While these sorts of methods are popular and do tend to perform well, the resulting features are usually not interpretable \cite{Guyon}. 

It might be of interest to see which features are most responsible for differences between classes. Therefore, instead of using feature transformation methods, feature selection is used to find a subset of features for which a classifier achieves its best performance. There exist numerous methods for performing feature selection, such as nested subset methods, filters or direct objective optimisation \cite{Guyon}, as well as adaptive boosting \cite{Wang_2}.

We use \textit{sequential floating selection} (SFS) \cite{Somol} to find the optimal set of features. SFS is a greedy algorithm that works by: Starting with an empty list, sequentially considering each feature and assessing its impact on a given evaluation score. It then chooses the feature that scores best and adds it to the list. Then, it goes through each of the features that have not been added to the list, and assess their performance in combination with the features already added to the list. It find the feature that improves performance most and adds it to the list. This is repeated until the list is full \cite{Juha}. A superior method, \textit{sequential forward floating selection}, has been proven to perform better \cite{Somol}, which backtracks after a new feature is included to solve the \textit{nesting} problem, it proved inefficient to implement for the multi class.

\subsection{Implementation}
Since it is not necessarily the case that the best features are the same for each classification proplem, or even for each classification algorithm, the best features are found for each classifier irrespective of the others. The figure of merit for each, which is optimises the classifier is found by using cross-validation and training a classification model model with training data and then evaluating it on a validation set. If at any stage, the feature being considered improves the figure of merit, then the feature will be added to the set of `kept' features.

Different evaluation scores are used depending on the classifier. In the k-nearest neighbors classifiers, the \textit{mincost} is, which is the predicted label with the smallest expected misclassification cost. The expectation is taken over the posterior probability, and cost as given by the Cost property of the classifier (a matrix). The loss is then the true misclassification cost averaged over the observations. For the random forest implementation, the cumulative misclassification probability of the entire ensemble is used as the cost to evaluate combination of features. In the case of logistic regression, the deviance of the fit is used. These methods were used for two reasons, firstly because efficient implementations exist with MATLAB's stats toolkit, and they produced the sets of features that performed best on when tested on a validation set.