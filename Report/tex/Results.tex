\chapter{Results}

This section discusses the quantitative evaluation methods used to determine the potential for each of the classifiers to reveal household characteristics and then analyses the results from training and running each classifier.

\section{Evaluation Methods}
\label{sec:evalMethods}
For each classifier, a \textit{confusion matrix} (CM) is produced using the MATLAB tool \texttt{confusionmat}, which, for a $K$ class classification problem, returns a $K\times K$ matrix where each element $(i,j)$ contains the number of times an instance of class $i$ has been classified as $j$. The diagonal elements elements of CM contain the number of instances of households that have been classified correctly for each class. \cite{Stefanowski}


The accuracy of a classifier is defined as the sum of the diagonal elements of CM, divided by the total number of samples,$S$.

\[ACC=\frac{\sum_{i=1}^KCM_{i,i}}{S}\]

This is compared to the accuracy of performing a random guess (RG), which assigns a household to one of the $K$ classes at random.
\[ACC_{RG}=\frac{1}{K}\]

To account for the imbalances in classes, we also calculate the most probable class (MPC) which uses knowledge of the prior probability of each class in the training data to find a baseline by assigning all samples to the most probable class. 

\[ACC_{MPC}=\frac{argmax(S^K)}{S}\]
where $S^K$ is the number of samples from the test data that are in class $K$.

\bigskip

For socio-economic classification problem, the ordinal structure of the classes should also be taken into account i.e it is worse for our classifier to predict a household of social grade B as D, then it is to predict it as C1 or A. Therefore, the \textit{accuracy within n}\cite{Gaudette}. 


Particularly for unbalanced classes, reporting the accuracy alone is not satisfactory in determining the quality of a classifier. The obvious and well known example being; constructing a classification problem where 99\% of instances are in class A and only 1\% in class B. A classifier that simply predicts all new data as class A would be correct 99\% of the time, but would still not be a good classifier. 

A widely applied method for evaluating a classifier is to compute the \textit{true positive rate} (TPR) and \textit{true negative rate}(TNR). The TPR gives the proportion of positives that are correctly identified as being positive, while the TNR gives the porportion of negatives that are correctly identified as negative.

\begin{align*}
TPR&=\frac{TP}{TP+FN}   &   TNR&=\frac{TN}{TN+FP}   
\end{align*}

From these statistics, it is common to plot an ROC curve, which is a plot of the TPR against the \textit{false positive rate}(FPR), which is defined as 1-TNR. The evaluation criterion (the area under the ROC curve) is preferred over the accuracy, particularly when considering unbalanced classes as the impact of skewness can be analysed \cite{Waegeman}. To create the ROC curve, a value is found for each classifier which acts as the threshold above which an instance is classified as positive. Typically for logistic regression, this is the probability of an instance being assigned to class 1.

This is not as straight forward for random forests and knn as they are not probabilistic classifiers. Probabilities can, however be generated from the classifier results. For random forest the decision boundary may be the ratio of number of trees that vote in favor of assigning an unseen instance to class 1 and the total number of trees. In knn it is the number of nearest neighbors that are of class 1 divided by the total number of nearest neighbors.

In computing the ROC curve to evaluate the binary classification task of discriminating between households with and without children is straight forward, it is straightforward te determine which class is `positive' and which is `negative'. However for multi-class classification it is unclear what is `positive' and what is `negative'. When evaluating their socio-economic classifier, Beckel et. al. group nearby groups together and then use a one-versus-all approach\cite{Beckel_2,Beckel_3}. A similar method is used, analogous to the \textit{accuracy within n} method described above, where classes within $n$ are considered positive and all else are negative.

The final metric that is presented is the Matthews correlation coefficient (MCC) which is a value between -1 and +1 representing the correlation between the predicted and true values of a binary classifier. A MCC of -1 indicates that that there is no correlation between the predicted a true class while a value of +1 would indicate a perfect classifier while a value of 0 means it is no better or worse than a random guess. MCC is a worthwhile metric to report as it gives a value to the performance without inflating the imbalances in class sizes \cite{Powers}. 

Let $X,Y$ each be an $S\times N$ matrix where $S$ is the number households and $N$ is the number classes. $X_s,n=1$ if a sample $s$ is predicted to be the $n^{th}$ class and 0 otherwise, $Y_s,n=1$ if a sample $s$ belongs to the $n^{th}$ class and 0 otherwise. The covariance of $X$ and $Y$ can then be written as

\[cov(X,Y)=\frac{1}{N}\sum^S_{s=1}\sum^N_{n=1}(X_{s,n}-\bar{X_n})((Y_{s,n}-\bar{Y_n})\]

where $\bar{X_n}$ and $\bar{Y_n}$ are the means of the $n^th$ columns of $X$ and $Y$ respectively.

The MCC is then defined as,

\[MCC=\frac{cov(X,Y)}{\sqrt{cov(X,X)\cdot cov(Y,Y)}}\]


For binary classification this can be interpreted as,

\[MCC=\frac{TP\times TN-FP\times FN}{\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}\]

While the MCC is mot commonly used when assessing binary classifiers, based on the definition of the MCC above, it can be extended to multi-class problems as described by Gorodkin\cite{Gorodkin}. If $C$ the confusion matrix of a given classifier, then the MCC is given as,

\[MCC=\frac{\sum\limits^N_{k,l,m=1}C_{k,k}C_{m,l}-C_{l,m}C_{k,m}}{\sqrt{\sum\limits^N_{k=1}\bigg[\bigg(\sum\limits^N_{l=1}C_{l,k}\bigg)\bigg(\sum\limits^N_{f\ne k,g=1}C_{g,f}\bigg)\bigg]}\cdot \sqrt{\sum\limits^N_{k=1}\bigg[\bigg(\sum\limits^N_{l=1}C_{k,l}\bigg)\bigg(\sum\limits^N_{f\ne k,g=1}C_{f,g}\bigg)\bigg]}}\] 

%\begin{itemize}
%\item confusion matrix
%\item accuracy
%\item For ordinal data -- accuracy within n (i.e accuracy +- one class) ans mean absolute error 
%\item baseline - ($ACC_{RG}$ - random guess and ($ACC_{BRG}$ -biased random guess)
%\item TPD and FPR
%\item ROC curve
%\item explain how cross validation was used
%\end{itemize}

\section{Feature Selection}

As explained in section \ref{sec:dimensionalityReduction}, SFS was used to determine which features are of greatest value for each classifier. What was noticed, however, is that when running the SFS algorithm multiple times, the selected features were not always the same for each of the classifiers (even with cross validation). This was seen particularly in random forest (as might be expected for an algorithm that uses bootstrap aggregation).  Therefore, the feature selection algorithm was run multiple times and the features that appeared most often were used. To evaluate the feature selection method, two additional sets of features were made (one for each classification problem) by choosing features based on how they appeared to seperate classes in \ref{sec:creatingFeatures}. All classification models are evaluated using the features found by SFS as well as the features found manually (labled as MAN). The lists of features used for each classifier are outlined in tables \ref{tab:childFeatures} and \ref{tab:socioFeatures}. 

\childFeatures
\socioFeatures

\section{Classification Results}

Here the results of the classifiers are presented of running each model on unseen data as outlined in \ref{sec:evalMethods}. First the results of discriminating between households with and without children is presented, then the results of the socio-economic classifiers are shows. In the next section, we will the results are discussed to determine which classifier, and set of features, performs best on a given task and compared to the results obtained by similar studies..

The same training and test sets are used for each classifier to ensure the that the results are fair. While the training set is the same that had been used in cross-validation to optimize the classifiers, the test data is completely unused until this point.

\subsection{Children vs No Children}

\childAccuracy

The results from Figure \ref{fig:childAccuracy} show the accuracy of each classifier built to discriminate between households with and without children. All classifiers perform better than a biased random guess with logistic regression with features selected using SFS predicted the greatest percentage of households correctly. Furthermore Figure \ref{fig:childMat} gives the Matthews correlation coefficients of each of the classifier which all take on positive values, meaning that the classifiers are less inclined to miss-classify a household regardless of which class has a higher prior probability. 

While the logistic regression classifier using the SFS features classifier gave both the highest accuracy and MCC, the performance is not significantly better than random forest and Knn. What is noticeable, is that all classifiers performed better when trained with features found by SFS features compared with manually feature selection, with the accuracy of all three classifiers that used SFS having an accuracy of above 80\% and all those with manyally selected features being below 80\%. The distinction is even more pronounced when considering the MCCs of each of the classifiers, where all classifiers using SFS features have an MCC of over 0.5, while none of the ones using manually selected features do. In the case of logistic regression, there is the greatest difference in performance with the SFS classifier having an MCC of 0.64 and the MAN classifier only having an MCC of 0.42. 

%Given that different feature selection methods have been used, it is worthwile to see which features contributed most to the classifiers such. With the logistic regression, we can compare the $\textbf{w}$ coefficients.
%\begin{align*}
%\textbf{w}_{MAN}= \begin{bmatrix}-14.9900\\ 0.622\\-1.3699\\-1.1673\\    1.1556\\-0.5732\\1.9194\\0.4214\\0.4015\\1.7463\\ 0.0000\end{bmatrix}
%&
%\textbf{w}_{SFS}= \begin{bmatrix}-3.2123 \\  -2.9622  \\ -1.4517   \\ 0.6498 \\   0.8683  \\  1.5765  \\  1.7768    \\     0  \\  0.5442  \\  1.7357  \\ -1.3840
%\end{bmatrix}
%\end{align*}
%
%What this shows, is that while the model using MAN features relies largely in the prior probability of each class, the model using SFS features relies less on the prior probability and more on on other features.
\childMat

\childROC
The ROC curves shown in figure \ref{fig:childROC} shows how the TPR changes when varying the FPR. Again, it can be seen that SFS features generate better results than MAN features and that, again, the logistic regression classifier gives the best performance (indicated by the area under the curve). Especially, for Knn, the TPR quickly increases with little increase in FPR, however once the specifificity (1-FPR) decreases, the performance of the random forest and logistic regression classifier become more accurate than knn.
\childResults

Finally, the table below shows the TPR and TNR for each of the classifiers along with the other statistics discussed previously. While the logistic repression classifier did have the greatest accuracy and MCC, it does not have the highest TPR, meaning that it was not best at identifying positive instances (households without children), as opposed to the Knn classifier which was able to identify 88.6\% of households without children. Looking at the TNR of each of the classifiers, again, Knn gives the best results, identifying 88.6\% of households that have children when using the SFS features, compared to only 79\% using a random forest classifier. This indicates, that although the logistic regression classifier had the greatest accuracy, Knn is better at correctly identifying the households from those without. From looking at the TNR, it can also be seen that the models built using MAN features rely much more on the imbalances in the sample sizes whereas models trained using SFS features are able to perform better than just a random guess.

\subsection{Socio-Economic Group}


While the results from the precious section showed that it is possible to discriminate between households with and without children, the classifiers built to determine a household's socio-economic status are not as promising.  By looking at the confusion matrix generated by each classifier, as shown in Table \ref{tab:socioConf}, insight can be gained into how each of the classifiers predicts unseen data. The first thing that is noticed is that the ordinal logistic regression models are little better than a biased random guess. They predict almost all test instances as either class C1 or C2, which are the two classes with the highest prior probability based on the sample population ($p(C1)=0.38$,$p(C2)=0.25$). While the nominal logstic regression classifiers are not significantly more accurate, they are not as heavily biased towards the most probable classes. This indicates that the proportional odds assumption is not satisfied. 
\socioConf

From the confusion matrices it can also be seen that all of the classifiers give a very low probability of a household being in class D. This is not uprising as the figures outlined in Chapter \ref{ch:Features} suggested that it is difficult to  distinguish between households of socio-ecomic classes D,C1 and C2, and since D has a much lower prior probability that the other two, a probabilistic classifier is likely to assign an unseen instance to either class C1 or C2. The random forest classifier does, however, 

Looking at Figure \ref{fig:socioAllAcc}, it can be seen that, although the accuracy of the classifiers starts to quickly increase as more of its neighbors are considered positive, the benchmark accuracy also increases rapidly. This is because the most probably classes are in the middle of the odering and are therefore more likely to be a neighboring class neighbor.


\socioAllAcc

\socioMat

Figure \ref{fig:socioMat} shows the MMCs of each classifier. 
logistic regression performs poorly
MAN features tend to do better than SFS. This could likely be because SFS is a greedy algorithm and, particularly for multi-class problems with limited amounts of data to perform validation on,  a backwards step such as that used in SFFS could render a more optimal set of features. Random forests are less effected by this as they, in a sense perform as they determine which feature is best at each stage of being built. The ROC curves generated by the classifiers is given in Apendix A, however and confirm that the classifiers' performance is only useful when trying to extract households of the classes C1 and C2.