\chapter{Discussion}

In this chapter, the important results from Chapter 3 are discussed and summarized and we try to draw conclusions about the household classification problem.

There are several major discussion points that could be outlined on the basis of the evaluation of the experimental design and so this chapter is split into 3 sections:

\begin{itemize}
\item Features and feature selection
\item Classifiers
\item Comparison to related work
\end{itemize}


\section{Features}

A large proportion of the time spent on this project was dedicated to gaining an understanding of the dataset and constructing a set of features that provide reliable results. The goal was to have a set of features that capture as much of the variance of the data in as low of a dimensionality as possible whilst still remaining interpretable. Given that one of the two classification problems was accomplished with high accuracy and relatively low error, we can conclude that such a set of features does exist. Below are some of the key observations and questions regarding feature extraction and selection .

\begin{enumerate}
\item \textit{Time-frequency transformation does not capture the periodic structure of the data}

Despite numerous attempts to extract informative frequency amplitudes from time-series, no feature was found that was able to distinguish between classes of households. Various time intervals were considered, including 4-week periods, and each week and day separately. Amplitudes were also found using time-series where the weekend period had been removed to see if patterns emerged over working days but were disrupted by weekends. The result was that, although taking the fft identified periods in the energy use of households, these could not be attributed to any one class relative to the others. 

\item \textit{Sequential forward selection provides a better method of selecting features than manually searching for a set, for finding households with children but not to determining socio-economic class }

In the problem of discriminating between households with and without children, the classifiers that relied on features selected by SFS performed better than the features selected by hand on almost all accounts. The performance metric with the largest gap was the \textit{true negative rate}. Classifiers using SFS features had, on average, 28\% better results than those. In the socio-economic classification problem, it is argued that SFS performed worse than choosing features based on human intuition. While the accuracies are comparable, the MCCs' are lower for most of the classifiers built on SFS features. One explanation for this is that the sample sizes of each class are small, and more data would allow the models determine the best features with more confidence. Another explanation is that, because SFS is a greedy algorithm, it is subjective to the `nesting effect' , that once a feature has been added it can't be removed \cite{Guyon}.   	

Which features are selected by SFS is dependent on what cost function is used to penalise the classifiers. Numerous cost functions were tried for each of the classifiers including misclassification rate, mean absolute error and false positive rate. When testing on the validation set, it was found that using more holistic cost functions gave better performance. A holistic being one that gives not solely based on the number of misclassified points.


\item \textit{The optimum set of features is different for each classifier, even when tackling the the same classification problem}

Cross-validation was implemented such that the training set was randomly split into five sets each time a classifier was trained (with each class represented proportionately the same in each set). This resulted in, especially for the inferring socio-economic group, the SFS method learning a new set of `best' parameters each time SFS was run. This can be attributed to the limited sample size. A larger set of data would give more stability in the results. The exception to this being the random forest which relies on random selection of features when learning each branch. It can therefore be argued that it was redundant to implement additional feature selection methods in the random forest classifiers.

\end{enumerate}

\section{Classifiers}

While the majority of the time was spent understanding the data and extracting features, the classifiers play a crucial role in the outcome of the project.  The three classifiers, logistic regression, random forest and Knn are discussed and their implementation critically evaluated.

\subsection{Logistic Regression}

\begin{enumerate}
\item \textit{Outperforms random forest and Knn in discriminating between households with and without children}

With the greatest area under the ROC curve, along with highest accuracy and MCC, the logistic regression classifier is concluded to be the best-performing model for this specific binary classification model. While Knn was able to better identify the childless households, it fell short in identifying the households with children.  The logistic regression model did a better job at identifying households that have children present. 

It is possible that the underlying reason for the positive results is that households with children will tend to have more occupants, which is directly correlated to the amount of energy consumed by the household. However, after testing the correlation between the misclasified and the number of occupants of a household, no correlation was found.


\item \textit{Both ordinal regression and multinomial logistic regression underperform relative to random forest and Knn in socio-economic classification problem}

Despite being the only model that takes the ordered nature of socio-economic classes into account, the proportional odds model ultimately performed no better than a biased random guess, assigning households to either C1 or C2. While previous studies have shown links between the amount of energy consumed and disposable income (which is itself linked to the socio-economic class), analysis of the HES data as well as other studies on domestic energy consumption found that not just lifestyle, but also dwelling-specific factors contributed to the electricity consumption of households. This is suggestive that, while it is possible to infer a households socio-economic status, more sophisticated methods need to be used to account for these factors.
\end{enumerate}

\subsection{Random Forest}

\begin{enumerate}

\item \textit{Improved performance not guaranteed using SFS}

Indicated by the results of the multi-class problem, employing statistical methods to reduce the dimentionality of the features used by the random forest classifier don't give better results on test data.  

\end{enumerate}

\subsection{Knn}

\begin{enumerate}

\item \textit{The larger K is, the more skewed the tradeoff between TPR and FPR becomes}

During the optimisation phase, the number of nearest neighbors was adjusted to find the optimum value. As K was increased, the accuracy and TPR continued to increase whereas the TNR declined until performance was equivalent to a biased guess. This is due to the imbalances in class sizes and can be adjusted for by limiting the size of K. Conversely, using small values of K such as 1 or 2 also produced poor results because the classes are not separable. Using the 5 nearest neighbors to evaluate the class of a new point gave the best results curing cross-validation.

\item \textit{Euclidean distance gives the most reliable results}

MATLAB's builtin Knn predictor allows numerous different distance measures to be specifies inclusing euclidean, Hamming, Manhatten and Mahalonobis. Of these, euclidean distance was found to give the most best resutlts based on TPR and TNR tradeoff. Euclidean distance assumes that each dimension is equally weighted which is reasonable since feature selection methods were used to determine a subset of features that reduce noise.
\end{enumerate}


\section{Comparison to Previous work}

Comparing the results obtained here to those found by Beckel et al. \cite{Beckel_1, Beckel_2, Beckel_3} who used smart meter data from Irish households to predict the socio-economic class and presence or absence of children, the classifiers produced here perform better. While Beckel et al. were able to construct an SVM classifier that had an accuracy of 71\% and MCC of 0.32 in discriminating between households with and without children, the results here showed that it is possible to obtain an accuracy of 83.7\% and MCC of 0.64 using logistic regression --- a notable improvement. While Beckel et al. achieved similar accuracy in their attempt to predict socio-economic class, the maximum MCC they obtained was 0.19, whereas the random forest constructed here had an MCC of 0.4. While this is not a strong correlation, it does indicate stronger predictive power than previous attempts.


