
\chapter{Models}

\section{Overview}
There are several classification algorithms that can be used to perform supervised learning tasks and vary in their computational complexity, implementation and assumptions that they make about the distributions of the data \cite{Beckel_3}. Three well known methods are used to classify the data: Logistic regression, random forrest and k-nearest neighbour.

All three methods are examples of discriminative classifiers. The discriminative approach is appealing in that it it directly models $p(y|\textbf{x})$. Also, density estimation for the class-conditional distributions is a hard problem, particularly when $\textbf{x}$ is high dimensional, so if we are just interested in classification, then the generative approach may mean that we are trying to solve a harder problem than we need to\cite{Williams}.

\section{Logistic Regression}
For a binary classification problem $y\in \{0,1\}$, such as discriminating between households with children ($y=1$) and households without ($y=0$), the logistic regression model learns a weight vector $\textbf{w}$ such that given some new household with feature vector $\textbf{x}$, the posterior probability of that household being in class, $p(y=1|\textbf{x})=g(\textbf{x}; \textbf{w})$ where $g(x)$ is the logistic (or sigmoid) function.

\[g(\textbf{x}; \textbf{w})=\sigma(\textbf{x};\textbf{w})=\frac{1}{1-e^{-(b+\textbf{w}\cdot{\textbf{x}})}}\]

There are numerous advantages to using logistic regression for the household classification task. Firstly, logistic regression is interpretable. After the model has been trained and the weight vectors established, they can be used to determine how important each feature is to the classifier. Secondly, the confidence of a prediction can be inferred, resulting in interpretable results. There are, however, also drawbacks to logistic regression. Since the maximum likelihood function does not have a closed form solution, an iterative process must be used instead to learn the weights, which is not guaranteed to converge. 


\subsection{Multi-class Logistic Regression}

To extend the problem of logistic regression to the multi-class case, often times the the \textit{softmax} is used as a generalisation of the logistic function ($\sigma$), the predicted class of an instance is then given by

\[P(y=Y_i|\textbf{x})=\frac{\exp^{-(b_i+\textbf{w}_i\cdot{\textbf{x}})}}{\sum_{j=0}^J\exp^{-(b_j+\textbf{w}_j\cdot{\textbf{x}})}}\]

Although this is a valid method of classifying the data, it fails to acknowledge the ordinal property of the classes and assumes the data to be nominal. Ideally we would be able to build a model that exploits the fact that some classes are more similar than others. For example, if the true label of a household is B, then we would rather missclassify the instance as A or C1 than as D or E. Luckily, ordinal logistic regression (or ordered ligit) can be used to build a model that incorporated the ordering of the classes.

McCullagh's proportional odds model \cite{McCullagh} is a variation of a generalised linear model (glm) where the dependant variable is thought of as being continuous, but is recorded ordinally. It can be thought of as asking asking a linear model to tell you what range a dependent variable is in (as opposed to an exact value). The regression model is then given by 
\begin{align*}
logit(p_1) &=\log(\frac{p_1}{1-p_1}) = b_1 +\textbf{w}\cdot{\textbf{x}} \\
logit(p_2) &=\log(\frac{p_1+p_2}{1-p_1-p_2}) = b_2 +\textbf{w}\cdot{\textbf{x}}\\
&\vdots \\
logit(p_6) &=\log(\frac{p_1+p_2+p_3+p_4+p_5+p_6}{1-p_1-p_2-p_3-p_4-p_5-p_6}) = b_6 +\textbf{w}\cdot{\textbf{x}}
\end{align*}

This model relies on the \textit{proportional odds assumption}, which is that the $\textbf{w}$s are independent of the classes (hence they have no subscripts). This translates to the assumption that the weights are the same for each cutoff, but rather the classes have different intercepts $b$, in contrast to multinomial logistic regression (where the dependent variables are assumed to be nominal) which learns a new set of weight parameters for each cutoff point. A further description of regression models for ordinal data in the context of machine learning is given by Herbrich et. al. \cite{Herbrich}.

\orderedLogit
\subsection{Implementation}

To build the binary logistic regression classifier, MATLAB's fitglm tool which fits a generalised linear model to the data. Logit link fuctions and a binomial distribution are passed as parameters to the glmfit function which produces a logistic regression model. For the case of multi-class classification, two models were created. One using McCullagh's proportional odds model (treating the data as ordinal) and one using multinomial logistic regression (treating the data as nominal). Both methods were implemented in MATLAB using the mnrfit tool.

\section{Random Forest}

Random Forest is a classification method that grows an ensemble of decision trees from a set of training instances and determines the class of a new instance by allowing the trees in the ensemble to vote on the most popular class. For $N$ training sets and $M$ features, each tree is grown by:

\begin{itemize}
\item Randomly sample $n$ training instances from the $N$ training with replacement (this will be the training sample to grow the tree).
\item At each node, selecting $m$ features at random (where $m < M$), the best of which is used to split the node.
\item The trees are grown to the largest possible size (no pruning takes place).
\end{itemize}

A new instance is then classified by running it through each tree, allowing each of the trees to assign the instance a class. The predicted class of the test instance is then given by the vote of each tree.

Although (in contrast to building a single decision tree), it is not easy to visualise a random forest, it is still possible to gain an estimate of the variables that are most important for classification and can be used on data sets with a large number of features (see Section \ref{sec:dimensionalityReduction}). Random forests have been shown to perform particularly well on unseen data compared to other classification methods as they avoid overfitting by only ever looking at a random subset of features and data \cite{Breiman}. 

\subsection{Implementation}

MATLAB's built-in treeBagger class was used to build the random forest. Because bootstrap aggregation is used to randomly sample the training data, the out-of-bag (oob) estimates were used to optimise the model's parameters instead of using cross-validation. The parameters to optimise are the number of trees in the forest, and number of features $m$ to consider for splitting each node. It was found that for both classification tasks, optimum number of trees in the forest is 13, and that evaluating $\frac{M}{2}$ randomly selected at each node gives the best oob error.

\section{K-Nearest Neighbour}

K-nearest neighbour is a fundamental method for classification as it is intuitive and requires little \textit{a priori} knowledge about the data. It is a non-parametric model that classifies an unlabelled input by finding the K-nearest training points in feature space, using the classes of the nearest points to predict the class of the unlabelled point \cite{Peterson}. 

\subsection{Implementation}

MATLAB's fitknn tool was used to build a nearest neighbour classification model and the optimum parameters were found using 5-fold cross-validation. The parameters to find were the distance measure, search method and $k$ (the number of neighbours). Using the 5-nearest neighbours and euclidean distance gave the best results in cross-validation.

%\begin{itemize}
%\item number of instances is relatively small so parametric classifier won't be too expensive. 
%\end{itemize}
