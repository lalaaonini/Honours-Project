
\chapter{Models}

\section{Overview}
There are several classification algorithms that can be used to perform supervised learning tasks and vary in their computational complexity, implementation and assumptions that they make about the distributions of the data \cite{Beckel_3}. Three well known methods are used to classify the data: Logistic regression, random forrest and k-nearest neighbour.

All three methods are examples of discriminative classifiers. The discriminative approach is appealing in that it is directly modelling what we want, $p(y|\textbf{x})$. Also, density estimation for the class-conditional distributions is a hard problem, particularly when $\textbf{x}$ is high dimensional, so if we are just interested in classification then the generative approach may mean that we are trying to solve a harder problem than we need to\cite{Williams}. We are also fortunate in that there is no missing data. 

\section{Logistic Regression}
For a binary classification problem $y\in \{0,1\}$, such as discriminating between households with children ($y=1$) and households without ($y=0$), the logistic regression model learns a weight vector $\textbf{w}$ such that given some new household with feature vector $\textbf{x}$, the posterior probability of that household being in class, $p(y=1|\textbf{x})=g(\textbf{x}; \textbf{w})$ where $g(x)$ is the logistic (or sigmoid) function.

\[g(\textbf{x}; \textbf{w})=\sigma(\textbf{x};\textbf{w})=\frac{1}{1-e^{-(b+\textbf{w}\cdot{\textbf{x}})}}\]

There are numerous pros to using logistic regression for the household classification task. Firstly, logistic regression is interpretable. After the model has been trained and the weight vectors established, they can be used to determine how important each feature is to the classifier. Secondly, the confidence of a prediction can be inferred, resulting in interpretable results.


\begin{itemize}
\item interpretable -can look at the weights
\item gives a confidence in our predictions -probability
\end{itemize}

\subsection{Multi-class Logistic Regression}

To extend the problem of logistic regression to the multi-class case, often times the the \textit{softmax} is used as a generalisation of the logistic function ($\sigma$), the predicted class of an instance is then given by

\[P(y=Y_i|\textbf{x})=\frac{\exp^{-(b_i+\textbf{w}_i\cdot{\textbf{x}})}}{\sum_{j=0}^J\exp^{-(b_j+\textbf{w}_j\cdot{\textbf{x}})}}\]

Although this is a valid method of classifying the data, it fails to acknowledge the ordinal property of the classes and assumes the data to be nominal. Ideally we would be able to build a model that exploits the fact that some classes are more similar than others. For example, if the true label of a household is B, then we would rather misclassify the instance as A or C1 than as D or E. Luckily, ordinal logistic regression (or ordered ligit) can be used to build a model that incorporated the ordering of the classes.

Defining $p_i$ as the proportion of instances that are in class $i$, then the ordered logit model has the form
\begin{align*}
logit(p_1) &=\log(\frac{p_1}{1-p_1}) = b_1 +\textbf{w}\cdot{\textbf{x}} \\
logit(p_2) &=\log(\frac{p_1+p_2}{1-p_1-p_2}) = b_2 +\textbf{w}\cdot{\textbf{x}}\\
&\vdots \\
logit(p_6) &=\log(\frac{p_1+p_2+p_3+p_4+p_5+p_6}{1-p_1-p_2-p_3-p_4-p_5-p_6}) = b_6 +\textbf{w}\cdot{\textbf{x}}
\end{align*}

The model assumes \textit{proportional odds} which is that each explanatory variable exerts the same effect on each cumulative logit. This translates to the assumption that the weights are the same for each cutoff, but rather the classes have different intercepts $b$.
\subsection{Implementation}

\section{Random Forrest}


\section{K-Nearest Neighbour}


%\begin{itemize}
%\item number of instances is relatively small so parametric classifier won't be too expensive. 
%\end{itemize}
